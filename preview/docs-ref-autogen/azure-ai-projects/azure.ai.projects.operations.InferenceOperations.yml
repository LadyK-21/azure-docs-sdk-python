### YamlMime:PythonClass
uid: azure.ai.projects.operations.InferenceOperations
name: InferenceOperations
fullName: azure.ai.projects.operations.InferenceOperations
module: azure.ai.projects.operations
summary: "> [!WARNING]\n> DO NOT instantiate this class directly.\n>\n> \n>\n> Instead,\
  \ you should access the following operations through\n>\n> <xref:azure.ai.projects.AIProjectClient>'s\n\
  >\n> <xref:inference> attribute.\n>"
constructor:
  syntax: 'InferenceOperations(outer_instance: azure.ai.projects.AIProjectClient)'
  parameters:
  - name: outer_instance
    isRequired: true
methods:
- uid: azure.ai.projects.operations.InferenceOperations.get_azure_openai_client
  name: get_azure_openai_client
  summary: 'Get an authenticated AzureOpenAI client (from the *openai* package) to
    use with

    AI models deployed to your AI Foundry Project or connected Azure OpenAI services.


    > [!NOTE]

    > The package openai must be installed prior to calling this method.

    >'
  signature: 'get_azure_openai_client(*, api_version: str | None = None, connection_name:
    str | None = None, **kwargs) -> AzureOpenAI'
  keywordOnlyParameters:
  - name: api_version
    description: 'The Azure OpenAI api-version to use when creating the client. Optional.

      See "Data plane - Inference" row in the table at

      [https://learn.microsoft.com/azure/ai-services/openai/reference#api-specs](https://learn.microsoft.com/azure/ai-services/openai/reference#api-specs).
      If this keyword

      is not specified, you must set the environment variable *OPENAI_API_VERSION*
      instead.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: connection_name
    description: 'Optional. If specified, the connection named here must be of type
      Azure OpenAI.

      The returned AzureOpenAI client will use the inference URL specified by the
      connected Azure OpenAI

      service, and can be used with AI models deployed to that service. If not specified,
      the returned

      AzureOpenAI client will use the inference URL of the parent AI Services resource,
      and can be used

      with AI models deployed directly to your AI Foundry project.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  return:
    description: An authenticated AzureOpenAI client
    types:
    - <xref:openai.AzureOpenAI>
  exceptions:
  - type: azure.core.exceptions.ResourceNotFoundError
    description: 'if an Azure OpenAI connection

      does not exist.'
  - type: azure.core.exceptions.ModuleNotFoundError
    description: 'if the *openai* package

      is not installed.'
  - type: ValueError
    description: if the connection name is an empty string.
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.ai.projects.operations.InferenceOperations.get_chat_completions_client
  name: get_chat_completions_client
  summary: 'Get an authenticated ChatCompletionsClient (from the package azure-ai-inference)
    to use with

    AI models deployed to your AI Foundry Project. Keyword arguments are passed to
    the constructor of

    ChatCompletionsClient.


    At least one AI model that supports chat completions must be deployed.


    > [!NOTE]

    > The package azure-ai-inference must be installed prior to calling this method.

    >'
  signature: 'get_chat_completions_client(**kwargs: Any) -> ChatCompletionsClient'
  return:
    description: An authenticated chat completions client.
    types:
    - <xref:azure.ai.inference.ChatCompletionsClient>
  exceptions:
  - type: azure.core.exceptions.ModuleNotFoundError
    description: 'if the *azure-ai-inference* package

      is not installed.'
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.ai.projects.operations.InferenceOperations.get_embeddings_client
  name: get_embeddings_client
  summary: 'Get an authenticated EmbeddingsClient (from the package azure-ai-inference)
    to use with

    AI models deployed to your AI Foundry Project. Keyword arguments are passed to
    the constructor of

    ChatCompletionsClient.


    At least one AI model that supports text embeddings must be deployed.


    > [!NOTE]

    > The package azure-ai-inference must be installed prior to calling this method.

    >'
  signature: 'get_embeddings_client(**kwargs: Any) -> EmbeddingsClient'
  return:
    description: An authenticated Embeddings client.
    types:
    - <xref:azure.ai.inference.EmbeddingsClient>
  exceptions:
  - type: azure.core.exceptions.ModuleNotFoundError
    description: 'if the *azure-ai-inference* package

      is not installed.'
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.ai.projects.operations.InferenceOperations.get_image_embeddings_client
  name: get_image_embeddings_client
  summary: 'Get an authenticated ImageEmbeddingsClient (from the package azure-ai-inference)
    to use with

    AI models deployed to your AI Foundry Project. Keyword arguments are passed to
    the constructor of

    ChatCompletionsClient.


    At least one AI model that supports image embeddings must be deployed.


    > [!NOTE]

    > The package azure-ai-inference must be installed prior to calling this method.

    >'
  signature: 'get_image_embeddings_client(**kwargs: Any) -> ImageEmbeddingsClient'
  return:
    description: An authenticated Image Embeddings client.
    types:
    - <xref:azure.ai.inference.ImageEmbeddingsClient>
  exceptions:
  - type: azure.core.exceptions.ModuleNotFoundError
    description: 'if the *azure-ai-inference* package

      is not installed.'
  - type: azure.core.exceptions.HttpResponseError
