### YamlMime:PythonClass
uid: azure.ai.projects.aio.operations.InferenceOperations
name: InferenceOperations
fullName: azure.ai.projects.aio.operations.InferenceOperations
module: azure.ai.projects.aio.operations
summary: "> [!WARNING]\n> DO NOT instantiate this class directly.\n>\n> \n>\n> Instead,\
  \ you should access the following operations through\n>\n> <xref:azure.ai.projects.aio.AIProjectClient>'s\n\
  >\n> <xref:inference> attribute.\n>"
constructor:
  syntax: 'InferenceOperations(outer_instance: azure.ai.projects.aio.AIProjectClient)'
  parameters:
  - name: outer_instance
    isRequired: true
methods:
- uid: azure.ai.projects.aio.operations.InferenceOperations.get_azure_openai_client
  name: get_azure_openai_client
  summary: 'Get an authenticated AsyncAzureOpenAI client (from the *openai* package)
    to use with

    AI models deployed to your AI Foundry Project or connected Azure OpenAI services.


    > [!NOTE]

    > The package openai must be installed prior to calling this method.

    >'
  signature: 'async get_azure_openai_client(*, api_version: str | None = None, connection_name:
    str | None = None, **kwargs) -> AsyncAzureOpenAI'
  keywordOnlyParameters:
  - name: api_version
    description: 'The Azure OpenAI api-version to use when creating the client. Optional.

      See "Data plane - Inference" row in the table at

      [https://learn.microsoft.com/azure/ai-services/openai/reference#api-specs](https://learn.microsoft.com/azure/ai-services/openai/reference#api-specs).
      If this keyword

      is not specified, you must set the environment variable *OPENAI_API_VERSION*
      instead.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: connection_name
    description: 'Optional. If specified, the connection named here must be of type
      Azure OpenAI.

      The returned AzureOpenAI client will use the inference URL specified by the
      connected Azure OpenAI

      service, and can be used with AI models deployed to that service. If not specified,
      the returned

      AzureOpenAI client will use the inference URL of the parent AI Services resource,
      and can be used

      with AI models deployed directly to your AI Foundry project.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  return:
    description: An authenticated AsyncAzureOpenAI client
    types:
    - <xref:openai.AsyncAzureOpenAI>
  exceptions:
  - type: azure.core.exceptions.ResourceNotFoundError
    description: 'if an Azure OpenAI connection

      does not exist.'
  - type: azure.core.exceptions.ModuleNotFoundError
    description: 'if the *openai* package

      is not installed.'
  - type: ValueError
    description: if the connection name is an empty string.
  - type: azure.core.exceptions.HttpResponseError
